#!/usr/bin/env python3
"""
æµ‹è¯•tokenizationåŠŸèƒ½
éªŒè¯æ–‡æœ¬æ˜¯å¦æ­£ç¡®è½¬æ¢ä¸ºtoken IDs
"""

import torch
from transformers import BioGptTokenizer

def test_basic_tokenization():
    """æµ‹è¯•åŸºæœ¬çš„tokenizationåŠŸèƒ½"""
    
    # ä½¿ç”¨ä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„tokenizer
    commit_hash = 'eb0d815e95434dc9e3b78f464e52b899bee7d923'
    tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt', revision=commit_hash)
    
    print("BioGPT Tokenizerä¿¡æ¯:")
    print(f"Vocab size: {tokenizer.vocab_size}")
    print(f"Pad token ID: {tokenizer.pad_token_id}")
    print(f"BOS token ID: {tokenizer.bos_token_id}")
    print(f"EOS token ID: {tokenizer.eos_token_id}")
    print(f"EOS token: '{tokenizer.eos_token}'")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "The patient has a malignant tumor in the breast tissue.",
        "Histopathological examination reveals adenocarcinoma.",
        "Immunohistochemical staining is positive for ER and PR."
    ]
    
    print("\næµ‹è¯•tokenization:")
    for i, text in enumerate(test_texts):
        print(f"\næ–‡æœ¬ {i+1}: {text}")
        
        # æ·»åŠ EOS token
        text_with_eos = text + tokenizer.eos_token
        print(f"æ·»åŠ EOSå: {text_with_eos}")
        
        # Tokenize
        tokenized = tokenizer(
            text=text_with_eos,
            add_special_tokens=True,
            padding=False,
            return_tensors='pt',
            return_token_type_ids=False,
            return_attention_mask=False,
        )
        
        token_ids = tokenized['input_ids'][0]
        print(f"Token IDs: {token_ids.tolist()}")
        print(f"Tokenæ•°é‡: {len(token_ids)}")
        
        # è§£ç å›æ–‡æœ¬
        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
        print(f"è§£ç å: {decoded}")

def test_dataset_tokenization():
    """æµ‹è¯•æ•°æ®é›†ä¸­çš„tokenizationæ–¹æ³•"""
    
    # æ¨¡æ‹ŸWSIDatasetçš„tokenize_textæ–¹æ³•
    commit_hash = 'eb0d815e95434dc9e3b78f464e52b899bee7d923'
    tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt', revision=commit_hash)
    max_seq_len = 512
    
    def tokenize_text(text: str) -> torch.Tensor:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDs"""
        # æ·»åŠ EOS token
        text_with_eos = text + tokenizer.eos_token
        
        # Tokenize
        tokenized = tokenizer(
            text=text_with_eos,
            add_special_tokens=True,
            padding=False,
            return_tensors='pt',
            return_token_type_ids=False,
            return_attention_mask=False,
        )
        
        token_ids = tokenized['input_ids'][0]
        
        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(token_ids) > max_seq_len:
            token_ids = token_ids[:max_seq_len]
        
        return token_ids
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = "The patient has a malignant tumor in the breast tissue with invasive ductal carcinoma."
    
    print(f"\næµ‹è¯•æ•°æ®é›†tokenizationæ–¹æ³•:")
    print(f"è¾“å…¥æ–‡æœ¬: {test_text}")
    
    tokens = tokenize_text(test_text)
    print(f"è¾“å‡ºtokens: {tokens.tolist()}")
    print(f"Tokenæ•°é‡: {len(tokens)}")
    
    # æµ‹è¯•æ ‡ç­¾åˆ›å»º
    labels = tokens[1:].contiguous()  # å»æ‰BOS token
    input_tokens = tokens[:-1].contiguous()  # å»æ‰EOS token
    
    print(f"è¾“å…¥tokens (å»æ‰EOS): {input_tokens.tolist()}")
    print(f"æ ‡ç­¾tokens (å»æ‰BOS): {labels.tolist()}")
    print(f"è¾“å…¥é•¿åº¦: {len(input_tokens)}, æ ‡ç­¾é•¿åº¦: {len(labels)}")

def test_collate_function():
    """æµ‹è¯•collateå‡½æ•°"""
    
    # æ¨¡æ‹Ÿbatchæ•°æ®
    batch = [
        {
            'image_embeddings': torch.randn(1, 10, 2560),
            'text_tokens': torch.tensor([2, 21477, 2626, 7265, 5, 4532, 13893, 4, 2]),  # è¾ƒçŸ­çš„åºåˆ—
            'text': "Short text",
            'filename': "file1"
        },
        {
            'image_embeddings': torch.randn(1, 15, 2560),
            'text_tokens': torch.tensor([2, 21477, 2626, 7265, 5, 4532, 13893, 4, 2, 1234, 5678, 9]),  # è¾ƒé•¿çš„åºåˆ—
            'text': "Longer text example",
            'filename': "file2"
        }
    ]
    
    print(f"\næµ‹è¯•collateå‡½æ•°:")
    print(f"Batchå¤§å°: {len(batch)}")
    
    # æ¨¡æ‹Ÿcollate_fn
    # è·å–æœ€å¤§åºåˆ—é•¿åº¦
    max_seq_len = max(item['image_embeddings'].shape[1] for item in batch)
    print(f"æœ€å¤§å›¾åƒåºåˆ—é•¿åº¦: {max_seq_len}")
    
    # å¤„ç†å›¾åƒembeddings
    image_embeddings = []
    for item in batch:
        emb = item['image_embeddings']
        if emb.shape[1] < max_seq_len:
            # Padding
            pad_size = max_seq_len - emb.shape[1]
            emb = torch.cat([emb, torch.zeros(emb.shape[0], pad_size, emb.shape[2])], dim=1)
        image_embeddings.append(emb)
    
    image_embeddings = torch.cat(image_embeddings, dim=0)
    print(f"å›¾åƒembeddingså½¢çŠ¶: {image_embeddings.shape}")
    
    # å¤„ç†æ–‡æœ¬tokens
    text_tokens = [item['text_tokens'] for item in batch]
    
    # è·å–æœ€å¤§æ–‡æœ¬é•¿åº¦
    max_text_len = max(len(tokens) for tokens in text_tokens)
    print(f"æœ€å¤§æ–‡æœ¬é•¿åº¦: {max_text_len}")
    
    # Paddingæ–‡æœ¬tokens
    padded_text_tokens = []
    for tokens in text_tokens:
        if len(tokens) < max_text_len:
            # ä½¿ç”¨pad_idè¿›è¡Œpadding
            padding = torch.full((max_text_len - len(tokens),), 1, dtype=tokens.dtype)  # pad_id = 1
            tokens = torch.cat([tokens, padding])
        padded_text_tokens.append(tokens)
    
    text_tokens = torch.stack(padded_text_tokens)
    print(f"æ–‡æœ¬tokenså½¢çŠ¶: {text_tokens.shape}")
    print(f"æ–‡æœ¬tokenså†…å®¹:")
    for i, tokens in enumerate(text_tokens):
        print(f"  æ ·æœ¬{i+1}: {tokens.tolist()}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•tokenizationåŠŸèƒ½")
    print("=" * 50)
    
    test_basic_tokenization()
    test_dataset_tokenization()
    test_collate_function()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼") 