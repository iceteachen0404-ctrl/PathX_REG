{
  "auto_map": {
    "AutoConfig": "configuring_prism.PrismConfig",
    "AutoModel": "modeling_prism.Prism"
  },
  "biogpt_config": {
    "activation_dropout": 0.0,
    "architectures": [
      "BioGptForCausalLM"
    ],
    "attention_probs_dropout_prob": 0.1,
    "bos_token_id": 0,
    "eos_token_id": 2,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 1024,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-12,
    "layerdrop": 0.0,
    "max_position_embeddings": 1024,
    "model_type": "biogpt",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "pad_token_id": 1,
    "scale_embedding": true,
    "transformers_version": "4.25.0.dev0",
    "use_cache": true,
    "vocab_size": 42384
  },
  "biogpt_context_dim": 1280,
  "biogpt_frozen_weights": true,
  "biogpt_frozen_embeddings": false,
  "dim_latents": 5120,
  "is_composition": true,
  "model_type": "prism",
  "perceiver_config": {
    "context_dim": 2560,
    "latent_dim": 1280,
    "latent_seq": 512,
    "mhsa_heads": 8,
    "mlp_activation": "geglu",
    "mlp_mult": 1,
    "model_type": "perceiver",
    "perceiver_depth": 8,
    "share_tf_start_layer": 0,
    "share_xattn_start_layer": 1,
    "transformer_depth": 6,
    "xattn_heads": 1
  },
  "transformers_version": "4.42.4"
}
