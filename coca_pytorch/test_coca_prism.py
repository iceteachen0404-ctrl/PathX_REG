#!/usr/bin/env python3
"""
æµ‹è¯•CoCaPrismæ¨¡å‹
éªŒè¯åŸºäºPrismæ¶æ„çš„CoCaæ¨¡å‹åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path

# å¯¼å…¥CoCaPrismæ¨¡å‹
from coca_pytorch_prism import CoCaPrism

# å¯¼å…¥Prismé…ç½®
import sys
sys.path.append('./prism')
from prism.configuring_prism import PrismConfig

# å¯¼å…¥BioGPT tokenizer
from transformers import BioGptTokenizer

def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–...")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆä»HuggingFaceåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
    model = CoCaPrism(
        prism_model_name='/home/cjt/project_script/coca_pytorch/prism',
        caption_loss_weight=1.0,
        contrastive_loss_weight=1.0,
        frozen_img=True,
        frozen_text_weights=True,  # å†»ç»“é™¤Embeddingå’ŒCross-Attentionå±‚å¤–æ‰€æœ‰å±‚
        frozen_text_embeddings=False  # å†»ç»“Embeddingå±‚
    )
    
    print(f"CoCaPrismæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  - æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print(f"  - Prismé…ç½®: {model.prism_config}")
    print(f"  - Biogpt vocab_size: {model.vocab_size}")
    print(f"  - Biogpt hidden_size: {model.prism_config.biogpt_config.hidden_size}")
    print(f"  - Perceiver latent_dim: {model.prism_config.perceiver_config.latent_dim}")
    print(f"  - Dim latents: {model.prism_config.dim_latents}")
    
    return model

def test_tokenization():
    """æµ‹è¯•tokenizationåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•tokenizationåŠŸèƒ½...")
    
    # åˆå§‹åŒ–tokenizer
    commit_hash = 'eb0d815e95434dc9e3b78f464e52b899bee7d923'
    tokenizer = BioGptTokenizer.from_pretrained('/data/case_level/open_source_model/model/biogpt', revision=commit_hash)
    
    test_texts = [
        "The patient has a malignant tumor in the breast tissue.",
        "Histopathological examination reveals adenocarcinoma.",
        "Immunohistochemical staining is positive for ER and PR."
    ]
    
    print("æµ‹è¯•æ–‡æœ¬tokenization:")
    for i, text in enumerate(test_texts):
        # ä½¿ç”¨æ¨¡å‹çš„tokenizeæ–¹æ³•
        tokens = tokenizer(
            text=text + tokenizer.eos_token,
            add_special_tokens=True,
            padding=False,
            return_tensors='pt',
            return_token_type_ids=False,
            return_attention_mask=False,
        )
        
        token_ids = tokens['input_ids'][0]
        print(f"  æ–‡æœ¬ {i+1}: {text}")
        print(f"    Token IDs: {token_ids.tolist()}")
        print(f"    Tokenæ•°é‡: {len(token_ids)}")
        
        # è§£ç éªŒè¯
        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
        print(f"    è§£ç å: {decoded}")
    
    return tokenizer

def test_forward_pass(model, tokenizer):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 1
    num_tiles = 10
    tile_dim = 2560
    seq_len = 20
    
    # æ¨¡æ‹Ÿtile embeddings
    tile_embeddings = torch.randn(batch_size, num_tiles, tile_dim)
    
    # æ¨¡æ‹Ÿæ–‡æœ¬tokens
    vocab_size = model.vocab_size
    text_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # åˆ›å»ºæ ‡ç­¾
    labels = text_tokens[:, 1:].contiguous()
    input_tokens = text_tokens[:, :-1].contiguous()
    
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"  - tile_embeddings: {tile_embeddings.shape}")
    print(f"  - input_tokens: {input_tokens.shape}")
    print(f"  - labels: {labels.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(
            text=input_tokens,
            image_tokens=tile_embeddings,
            labels=labels,
            return_loss=True
        )
    
    print(f"è¾“å‡ºæ•°æ®:")
    print(f"  - logits: {output['logits'].shape}")
    print(f"  - text_embedding: {output['text_embedding'].shape}")
    print(f"  - image_embedding: {output['image_embedding'].shape}")
    print(f"  - image_latents: {output['image_latents'].shape}")
    print(f"  - sim: {output['sim'].shape}")
    print(f"  - loss: {output['loss'].item():.4f}")
    print(f"  - caption_loss: {output['caption_loss'].item():.4f}")
    print(f"  - contrastive_loss: {output['contrastive_loss'].item():.4f}")
    
    return output

def test_image_embedding(model):
    """æµ‹è¯•å›¾åƒç¼–ç åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å›¾åƒç¼–ç åŠŸèƒ½...")
    
    batch_size = 2
    num_tiles = 15
    tile_dim = 2560
    
    # æ¨¡æ‹Ÿtile embeddings
    tile_embeddings = torch.randn(batch_size, num_tiles, tile_dim)
    
    # æµ‹è¯•embed_imageæ–¹æ³•
    with torch.no_grad():
        image_out = model.embed_image(tile_embeddings)
    
    print(f"å›¾åƒç¼–ç è¾“å‡º:")
    print(f"  - image_embedding: {image_out['image_embedding'].shape}")
    print(f"  - image_latents: {image_out['image_latents'].shape}")
    
    return image_out

def test_text_embedding(model, tokenizer):
    """æµ‹è¯•æ–‡æœ¬ç¼–ç åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬ç¼–ç åŠŸèƒ½...")
    
    batch_size = 2
    seq_len = 15
    
    # æ¨¡æ‹Ÿæ–‡æœ¬tokens
    vocab_size = model.vocab_size
    input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # æ¨¡æ‹Ÿimage latents
    context_len = 512
    context_dim = 1280
    image_latents = torch.randn(batch_size, context_len, context_dim)
    
    # æµ‹è¯•embed_textæ–¹æ³•
    with torch.no_grad():
        text_out = model.embed_text(input_tokens, image_latents)
    
    print(f"æ–‡æœ¬ç¼–ç è¾“å‡º:")
    print(f"  - text_embedding: {text_out['text_embedding'].shape}")
    print(f"  - logits: {text_out['logits'].shape}")
    
    return text_out

def test_generation(model, tokenizer):
    """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½...")
    
    batch_size = 2
    num_tiles = 10
    tile_dim = 2560
    
    # æ¨¡æ‹Ÿtile embeddings
    tile_embeddings = torch.randn(batch_size, num_tiles, tile_dim)
    
    # æµ‹è¯•ç”Ÿæˆ
    with torch.no_grad():
        generated_tokens = model.generate(
            image_tokens=tile_embeddings,
            max_length=20,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            bos_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    print(f"ç”Ÿæˆç»“æœ:")
    print(f"  - generated_tokenså½¢çŠ¶: {generated_tokens.shape}")
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    for i, tokens in enumerate(generated_tokens):
        text = tokenizer.decode(tokens, skip_special_tokens=True)
        print(f"  æ ·æœ¬ {i+1}: {text}")
    
    return generated_tokens

def test_model_properties(model):
    """æµ‹è¯•æ¨¡å‹å±æ€§"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹å±æ€§...")
    
    print(f"æ¨¡å‹å±æ€§:")
    print(f"  - pad_id: {model.pad_id}")
    print(f"  - bos_id: {model.bos_id}")
    print(f"  - eos_id: {model.eos_id}")
    print(f"  - vocab_size: {model.vocab_size}")
    print(f"  - caption_loss_weight: {model.caption_loss_weight}")
    print(f"  - contrastive_loss_weight: {model.contrastive_loss_weight}")

def test_loss_calculation(model):
    """æµ‹è¯•æŸå¤±è®¡ç®—"""
    print("\nğŸ§ª æµ‹è¯•æŸå¤±è®¡ç®—...")
    
    batch_size = 4
    num_tiles = 10
    tile_dim = 2560
    seq_len = 20
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    tile_embeddings = torch.randn(batch_size, num_tiles, tile_dim)
    text_tokens = torch.randint(0, model.vocab_size, (batch_size, seq_len))
    
    # åˆ›å»ºæ ‡ç­¾
    labels = text_tokens[:, 1:].contiguous()
    input_tokens = text_tokens[:, :-1].contiguous()
    
    # è®¡ç®—æŸå¤±
    with torch.no_grad():
        output = model(
            text=input_tokens,
            image_tokens=tile_embeddings,
            labels=labels,
            return_loss=True
        )
    
    print(f"æŸå¤±è®¡ç®—ç»“æœ:")
    print(f"  - æ€»æŸå¤±: {output['loss'].item():.4f}")
    print(f"  - CaptionæŸå¤±: {output['caption_loss'].item():.4f}")
    print(f"  - ContrastiveæŸå¤±: {output['contrastive_loss'].item():.4f}")
    
    # éªŒè¯æŸå¤±æƒé‡
    expected_total = (
        model.caption_loss_weight * output['caption_loss'] + 
        model.contrastive_loss_weight * output['contrastive_loss']
    )
    print(f"  - æœŸæœ›æ€»æŸå¤±: {expected_total.item():.4f}")
    print(f"  - æŸå¤±è®¡ç®—æ­£ç¡®: {torch.allclose(output['loss'], expected_total)}")

def main():
    print("ğŸš€ å¼€å§‹æµ‹è¯•CoCaPrismæ¨¡å‹")
    print("=" * 60)
    
    try:
        # 1. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
        model = test_model_initialization()
        
        # 2. æµ‹è¯•tokenization
        tokenizer = test_tokenization()
        
        # 3. æµ‹è¯•å‰å‘ä¼ æ’­
        test_forward_pass(model, tokenizer)
        
        # 4. æµ‹è¯•å›¾åƒç¼–ç 
        test_image_embedding(model)
        
        # 5. æµ‹è¯•æ–‡æœ¬ç¼–ç 
        test_text_embedding(model, tokenizer)
        
        # 6. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        test_generation(model, tokenizer)
        
        # 7. æµ‹è¯•æ¨¡å‹å±æ€§
        test_model_properties(model)
        
        # 8. æµ‹è¯•æŸå¤±è®¡ç®—
        test_loss_calculation(model)
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CoCaPrismæ¨¡å‹åŠŸèƒ½æ­£å¸¸")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 